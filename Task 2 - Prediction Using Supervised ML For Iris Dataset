# type your code here
from __future__ import print_function, division
import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib as mpl
import matplotlib.pyplot as plt
%matplotlib inline
from warnings import filterwarnings
filterwarnings('ignore')
# import train-test split 
from sklearn.model_selection import train_test_split

# import 'stats'
from scipy import stats

# 'metrics' from sklearn is used for evaluating the model performance
from sklearn.metrics import mean_squared_error

# import functions to perform feature selection
#from mlxtend.feature_selection import SequentialFeatureSelector as sfs
from sklearn.feature_selection import RFE

# import function to perform linear regression
from sklearn.linear_model import LinearRegression

# import functions to perform cross validation
from sklearn.model_selection import LeaveOneOut
from sklearn.model_selection import cross_val_score
from sklearn.model_selection import KFold

import statsmodels
import statsmodels.api as sm
import statsmodels.stats.api as sms
from statsmodels.compat import lzip
from statsmodels.stats.outliers_influence import variance_inflation_factor 
from statsmodels.graphics.gofplots import qqplot
from statsmodels.stats.anova import anova_lm
from statsmodels.formula.api import ols
from statsmodels.tools.eval_measures import rmse
from statsmodels.formula.api import ols

from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.linear_model import Lasso, Ridge, LassoCV, RidgeCV 
from sklearn.feature_selection import RFE
from sklearn.ensemble import RandomForestRegressor, BaggingRegressor

from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score

#Plotting Parameters
plt.rcParams['figure.figsize'] = [15,8]


## THE SPARKS FOUNDATION.
## Data Science & Business Analytics Intern [GRIPMAR21]

### Task-2 >> Prediction Using Supervised ML
#### 1] Find the Optimum Number of Clusters?
#### 2] Representing the Clusters Visually
## KMEANS CLUSTERING
### Used Python Scikit Learn (Sklearn) Library to Build the Kmeans Model
## Source of Data :>> : https://bit.ly/3kXTdox

data = pd.read_csv('~/Documents/DSE - Great Lakes Study/The Sparks Foundation/Task 2/Iris.csv',index_col = 0)
data.head()

data.shape

data.info()

data.describe().T

data.isnull().sum()

print(data['Species'].value_counts())

sns.countplot(data['Species'])
plt.title('Frequency Distribution of Species', fontsize = 20)
plt.xlabel('Species', fontsize = 15)
plt.ylabel('Count', fontsize = 15)

plt.xticks(fontsize =13)
plt.yticks(fontsize =13)

plt.show()

print(data.corr())

ax = sns.heatmap(data.corr(), annot = True)
bottom,top = ax.get_ylim()
ax.set_ylim(bottom + 0.5, top -0.5)

X= data.iloc[:,:-1].values

#now applying the elbow method to find the optimum number of clusters
from sklearn.cluster import KMeans
Within_Cluster_Sum_of_Squares = []
for i in range(1,11):
    kmeans = KMeans(n_clusters = i, random_state = 10)
    kmeans.fit(X)
    
    # within cluster sum of squared error values
    Within_Cluster_Sum_of_Squares.append(kmeans.inertia_)

plt.plot(range(1,11), Within_Cluster_Sum_of_Squares, marker = 'o')

plt.title('The Elbow Method', fontsize = 16)
plt.xlabel('Number of clusters', fontsize = 14)
plt.ylabel('Within_Cluster_Sum_of_Squares', fontsize = 14)

plt.xticks(range(1,11),fontsize = 13)
plt.yticks(fontsize = 13)

plt.grid()
plt.show()

#from the above graph we can decide the optimum number of clusters 
#we get optimum number of clusters at a point where the 'y coordinate doesn't decrease sigificantly 
#hence we can safely choose 3 clusters as optimal number of clusters #From the Above graph we can clearly see the Elbow formation at 2 and 3, So selecting 3 as optimum Number of Clusters


#now applying the kmeans algorithm to our given dataset
km = KMeans(n_clusters = 3, random_state = 10)
y_kmeans = km.fit_predict(X)
km.labels_

plt.scatter(X[y_kmeans == 0, 0], X[y_kmeans == 0, 1], s = 100, c = 'red', label = 'Iris-setosa')
plt.scatter(X[y_kmeans == 1, 0], X[y_kmeans == 1, 1], s = 100, c = 'blue', label = 'Iris-versicolour')
plt.scatter(X[y_kmeans == 2, 0], X[y_kmeans == 2, 1], s = 100, c = 'green', label = 'Iris-virginica')

#Plotting the centroids of the clusters
plt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:,1], s = 100, c = 'yellow', label = 'Centroids')

plt.legend()
plt.show()


# Number of Iterations performed

km.n_iter_


# Adding the predictions to the dataframe  

data1 = data.copy()
data1['Predicted_Species'] = km.labels_

# Function to Plot the clusters 

def cluster_plot(x,y):
    plt.figure(figsize = (12,6))

    for i in range(0,km.n_clusters):
        plt.scatter(x, y, cmap = 'tab10_r', 
                    data= data1[data1['Predicted_Species'] == i],
                    s = 100, label = i)

    if x == 'SepalLengthCm' and y == 'SepalWidthCm':
        a = 0
        b = 1
    else:
        a = 2
        b = 3
    
    plt.scatter(km.cluster_centers_[:, a], km.cluster_centers_[:, b],
                s = 550, c = 'red', marker = '*', label = 'Centeroid')

    plt.title('Predicted Clusters', fontsize = 16)
    plt.xlabel(x, fontsize = 14)
    plt.ylabel(y, fontsize = 14)

    plt.xticks(fontsize = 13)
    plt.yticks(fontsize = 13)

    plt.legend(loc = 0,fontsize = 12)
    plt.show()
cluster_plot('SepalLengthCm','SepalWidthCm')
cluster_plot('PetalLengthCm','PetalWidthCm')
# Frequency Distribution of Predicted Clusters

print(data1['Predicted_Species'].value_counts())

plt.figure(figsize = (8,6))
sns.countplot(data1['Predicted_Species']);
plt.title('Frequency Distribution of Predicted Clusters', fontsize = 16)
plt.xlabel('Cluster Number', fontsize = 14)
plt.ylabel('Count', fontsize = 14)

plt.xticks(fontsize = 13)
plt.yticks(fontsize = 13)

plt.show()
# Finding the Errors

for i in data1['Species'].value_counts().index:
    
    print(i,'\n',data1.loc[data1['Species'] == i]['Predicted_Species'].value_counts(),'\n')
#Since we can see that for Iris-versicolor the majority prediction is cluster 0 For Iris-setosa it is cluster 1 while for Iris-virginica it is cluster 2

# Mapping to the cluster number based on majority prediction

data1['Actual_Species'] = data1['Species'].map({'Iris-versicolor':0, 'Iris-setosa':1, 'Iris-virginica':2})
data1.head()

from sklearn.metrics import confusion_matrix, classification_report

print(confusion_matrix(data1['Actual_Species'], data1['Predicted_Species']),'\n\n')
print('Classification Report\n\n0 -> Iris-versicolor\n1 -> Iris-setosa\n2 -> Iris-virginica\n')
print(classification_report(data1['Actual_Species'], data1['Predicted_Species']))

